import requests
from bs4 import BeautifulSoup
import time
import socket
import socks
from fake_useragent import UserAgent
import concurrent.futures
import json
from datetime import datetime

# åˆå§‹åŒ–é…ç½®
ua = UserAgent()

# ä»£ç†ç½‘ç«™é…ç½®
SITE_CONFIGS = {
    'proxy_scdn': {
        'url': 'https://proxy.scdn.io/text.php',
        'type': 'api',
        'parser': 'pre',
        'extractor': lambda text: [
            {
                'ip': line.split(':')[0],
                'port': line.split(':')[1].strip(),
                'type': 'å¾…æ£€æµ‹',
                'https': 'å¾…æ£€æµ‹'
            } for line in text.split('\n') if ':' in line
        ]
    },
    'freeproxylist': {
        'url': 'https://free-proxy-list.net/',
        'type': 'web',
        'parser': 'table.table tbody tr',
        'extractor': lambda row: {
            'ip': row.find_all('td')[0].get_text() if len(row.find_all('td')) > 0 else None,
            'port': row.find_all('td')[1].get_text() if len(row.find_all('td')) > 1 else None,
            'type': 'SOCKS5' if 'socks' in (
                row.find_all('td')[4].get_text().lower() if len(row.find_all('td')) > 4 else '') else 'HTTP',
            'https': 'æ”¯æŒ' if 'yes' in (
                row.find_all('td')[6].get_text().lower() if len(row.find_all('td')) > 6 else '') else 'ä¸æ”¯æŒ'
        }
    },
    'proxy_list_plus': {
        'url': 'https://list.proxylistplus.com/Fresh-HTTP-Proxy-List-{page}',
        'pages': 5,
        'type': 'web',
        'parser': 'table.bg tr.cells',
        'extractor': lambda row: {
            'ip': row.find_all('td')[1].get_text(),
            'port': row.find_all('td')[2].get_text(),
            'type': row.find_all('td')[6].get_text(),
            'https': 'æ”¯æŒ' if 'yes' in row.find_all('td')[5].get_text().lower() else 'ä¸æ”¯æŒ'
        }
    },
    'geonode': {
        'url': 'https://proxylist.geonode.com/api/proxy-list?limit=100&page={page}',
        'pages': 5,
        'type': 'api',
        'extractor': lambda data: [
            {
                'ip': item['ip'],
                'port': item['port'],
                'type': item['protocols'][0].upper(),
                'https': 'æ”¯æŒ' if 'https' in item['protocols'] else 'ä¸æ”¯æŒ'
            } for item in data['data']
        ]
    },
    '89ip': {
        'url': 'https://www.89ip.cn/index_{page}.html',
        'pages': 5,
        'type': 'web',
        'parser': 'table.layui-table tbody tr',
        'extractor': lambda row: {
            'ip': row.find_all('td')[0].get_text().strip(),
            'port': row.find_all('td')[1].get_text().strip(),
            'type': 'HTTP',
            'https': 'å¾…æ£€æµ‹'
        }
    },
    'proxyscrape': {
        'url': 'https://api.proxyscrape.com/v2/?request=getproxies&protocol=http&timeout=10000&country=all&ssl=all&anonymity=all',
        'type': 'api',
        'extractor': lambda text: [
            {
                'ip': line.split(':')[0],
                'port': line.split(':')[1].strip(),
                'type': 'HTTP',
                'https': 'å¾…æ£€æµ‹'
            } for line in text.split('\n') if ':' in line
        ]
    },
    'sslproxies': {
        'url': 'https://www.sslproxies.org/',
        'type': 'web',
        'parser': 'table.table tbody tr',
        'extractor': lambda row: {
            'ip': row.find_all('td')[0].get_text() if len(row.find_all('td')) > 0 else None,
            'port': row.find_all('td')[1].get_text() if len(row.find_all('td')) > 1 else None,
            'type': 'HTTPS',
            'https': 'æ”¯æŒ'
        }
    }
}


# æ£€æµ‹åœ°åŒº
def detect_region(ip: str) -> str:
    max_retries = 3
    retries = 0
    while retries < max_retries:
        try:
            api_url = f"https://apimobile.meituan.com/locate/v2/ip/loc?rgeo=true&ip={ip}"
            response = requests.get(api_url, timeout=5)
            data = response.json()
            rgeo = data.get('data', {}).get('rgeo', {})
            country = rgeo.get('country', 'æœªçŸ¥')
            province = rgeo.get('province', 'æœªçŸ¥')
            city = rgeo.get('city', 'æœªçŸ¥')
            return f"{country}/{province}/{city}"
        except (requests.RequestException, json.JSONDecodeError):
            print(f"æ£€æµ‹ IP {ip} åœ°åŒºä¿¡æ¯å¤±è´¥ï¼Œå°è¯•ç¬¬ {retries + 1} æ¬¡é‡è¯•")
            retries += 1
    return "æœªçŸ¥"


# æ£€æµ‹ä»£ç†ç±»å‹
def detect_proxy_type(proxy: str) -> str:
    ip, port = proxy.split(':')
    test_urls = [
        ('http://httpbin.org/ip', 'HTTP'),
        ('https://httpbin.org/ip', 'HTTPS')
    ]

    for url, ptype in test_urls:
        try:
            proxies = {ptype.lower(): f'{ptype.lower()}://{proxy}'}
            res = requests.get(url, proxies=proxies, timeout=5)
            if res.status_code == 200:
                return ptype
        except:
            pass

    try:
        socks.set_default_proxy(socks.SOCKS5, ip, int(port))
        socket.socket = socks.socksocket
        res = requests.get('http://httpbin.org/ip', timeout=5)
        if res.status_code == 200:
            return 'SOCKS5'
    except:
        pass

    return 'æœªçŸ¥'


# æ ¸å¿ƒçˆ¬å–é€»è¾‘
def get_proxies_from_site(site: str) -> list:
    config = SITE_CONFIGS[site]
    all_proxies = []
    try:
        for page in range(1, config.get('pages', 1) + 1):
            headers = {'User-Agent': ua.random}
            url = config['url'].format(page=page) if '{page}' in config['url'] else config['url']
            print(f"æ­£åœ¨çˆ¬å– {site} ç¬¬ {page} é¡µ...")
            if config['type'] == 'api':
                response = requests.get(url, headers=headers, timeout=10)
                if site in ['proxy_scdn', 'proxyscrape']:
                    text = response.text
                    proxies = config['extractor'](text)
                else:
                    try:
                        data = response.json()
                        proxies = config['extractor'](data)
                    except json.JSONDecodeError:
                        print(f"è­¦å‘Šï¼š{site} ç¬¬ {page} é¡µè¿”å›çš„æ•°æ®æ ¼å¼æœ‰è¯¯ï¼Œè·³è¿‡æ­¤é¡µã€‚")
                        continue
            else:
                response = requests.get(url, headers=headers, timeout=10)
                soup = BeautifulSoup(response.text, 'html.parser')
                rows = soup.select(config['parser'])
                proxies = [config['extractor'](row) for row in rows if len(row.find_all('td')) >= 2]
            all_proxies.extend([proxy for proxy in proxies if proxy['ip'] and proxy['port']])
            time.sleep(1)
    except Exception as e:
        print(f"{site} çˆ¬å–å¤±è´¥: {e}")
    return all_proxies


# ä¿å­˜ä»£ç†
def save_proxies(proxies, format_type, filename, only_ip=False):
    if format_type == 'json':
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(proxies, f, ensure_ascii=False, indent=4)
    elif format_type == 'txt':
        with open(filename, 'w', encoding='utf-8') as f:
            if only_ip:
                for proxy in proxies:
                    f.write(f"{proxy['ip']}:{proxy['port']}\n")
            else:
                f.write("åœ°åŒº | ç±»å‹ | HTTPS | ä»£ç†åœ°å€\n")
                f.write("-" * 50 + "\n")
                for p in proxies:
                    p['addr'] = f"{p['ip']}:{p['port']}"
                    f.write(f"{p.get('region', 'æœªçŸ¥')} | {p['type']} | {p['https']} | {p['addr']}\n")


# éªŒè¯ä»£ç†å¯ç”¨æ€§
def validate_proxy(proxy):
    max_retries = 3
    retries = 0
    while retries < max_retries:
        try:
            test_url = 'http://httpbin.org/ip'
            proxy['addr'] = f"{proxy['ip']}:{proxy['port']}"
            proxies = {proxy['type'].lower(): f"{proxy['type'].lower()}://{proxy['addr']}"}
            timeout = 5 if proxy['type'] in ['HTTP', 'HTTPS'] else 10
            response = requests.get(test_url, proxies=proxies, timeout=timeout)
            if response.status_code == 200:
                print(f"ä»£ç† {proxy['addr']} éªŒè¯é€šè¿‡ï¼Œç±»å‹: {proxy['type']}ï¼Œæ˜¯å¦æ”¯æŒ HTTPS: {proxy['https']}")
                return proxy
            print(f"ä»£ç† {proxy['addr']} éªŒè¯å¤±è´¥ï¼Œå°è¯•ç¬¬ {retries + 1} æ¬¡é‡è¯•")
        except requests.RequestException as e:
            print(f"ä»£ç† {proxy['addr']} éªŒè¯å¤±è´¥ï¼Œé”™è¯¯ä¿¡æ¯: {e}ï¼Œå°è¯•ç¬¬ {retries + 1} æ¬¡é‡è¯•")
        except Exception as e:
            print(f"ä»£ç† {proxy['addr']} éªŒè¯å¤±è´¥ï¼ŒæœªçŸ¥é”™è¯¯: {e}ï¼Œå°è¯•ç¬¬ {retries + 1} æ¬¡é‡è¯•")
        retries += 1
    print(f"ä»£ç† {proxy['addr']} éªŒè¯å¤±è´¥ï¼Œè¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°")
    return None


# è¡¥å……ä»£ç†ä¿¡æ¯
def analyze_proxy(proxy, check_region, check_type, check_https):
    proxy['addr'] = f"{proxy['ip']}:{proxy['port']}"
    if check_region and proxy.get('region') is None:
        proxy['region'] = detect_region(proxy['ip'])
        print(f"ä»£ç† {proxy['addr']} åœ°åŒºæ£€æµ‹ç»“æœ: {proxy['region']}")
    if check_type and proxy['type'] in ['å¾…æ£€æµ‹', 'æœªçŸ¥']:
        proxy['type'] = detect_proxy_type(proxy['addr'])
        print(f"ä»£ç† {proxy['addr']} ç±»å‹æ£€æµ‹ç»“æœ: {proxy['type']}")
    if check_https and proxy['https'] == 'å¾…æ£€æµ‹':
        proxy['https'] = 'æ”¯æŒ' if proxy['type'] == 'HTTPS' else 'ä¸æ”¯æŒ'
        print(f"ä»£ç† {proxy['addr']} æ˜¯å¦æ”¯æŒ HTTPS æ£€æµ‹ç»“æœ: {proxy['https']}")
    return proxy


# ä¸»ç¨‹åº
def main():
    print("=" * 50)
    print("ğŸŒŸ å¤šæºä»£ç† IP çˆ¬è™« v3.0")
    print("=" * 50)

    # çˆ¬å–æ‰€æœ‰ä»£ç†
    all_proxies = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=6) as executor:
        futures = {executor.submit(get_proxies_from_site, site): site for site in SITE_CONFIGS}
        for future in concurrent.futures.as_completed(futures):
            site = futures[future]
            try:
                proxies = future.result()
                print(f"ä» {site} è·å–åˆ° {len(proxies)} ä¸ªä»£ç†")
                all_proxies.extend(proxies)
            except Exception as e:
                print(f"{site} çˆ¬å–å¼‚å¸¸: {e}")

    # å»é‡
    unique_proxies = []
    seen = set()
    for p in all_proxies:
        key = f"{p['ip']}:{p['port']}"
        if key not in seen:
            seen.add(key)
            unique_proxies.append(p)
    print(f"\nğŸ“¦ å…±è·å–åˆ° {len(unique_proxies)} ä¸ªå”¯ä¸€ä»£ç†")

    # è¯¢é—®ç”¨æˆ·éœ€è¦å¤šå°‘ä»£ç†
    while True:
        input_num = input("è¯·è¾“å…¥ä½ éœ€è¦çš„ä»£ç†æ•°é‡ï¼ˆç›´æ¥å›è½¦ä»£è¡¨å…¨éƒ¨ï¼‰ï¼š")
        if input_num == '':
            selected_proxies = unique_proxies
            break
        try:
            num = int(input_num)
            if num > len(unique_proxies):
                print(f"è¾“å…¥çš„æ•°é‡è¶…è¿‡äº†å¯ç”¨ä»£ç†æ•°é‡ï¼Œå¯ç”¨ä»£ç†æ•°é‡ä¸º {len(unique_proxies)}ï¼Œè¯·é‡æ–°è¾“å…¥ã€‚")
            else:
                selected_proxies = unique_proxies[:num]
                break
        except ValueError:
            print("è¾“å…¥æ— æ•ˆï¼Œè¯·è¾“å…¥ä¸€ä¸ªæœ‰æ•ˆçš„æ•´æ•°æˆ–ç›´æ¥å›è½¦ã€‚")

    # è·å–å½“å‰æ—¶é—´
    current_time = datetime.now().strftime("%Y%m%d%H%M%S")

    # ä¿å­˜æœªè¿‡æ»¤çš„ä»£ç† IP
    while True:
        format_choice = input("è¯·é€‰æ‹©æœªè¿‡æ»¤ä»£ç† IP çš„è¾“å‡ºæ ¼å¼ (json/txt): ").strip().lower()
        if format_choice in ['json', 'txt']:
            unfiltered_filename = f'unfiltered_proxies_{current_time}.{format_choice}'
            save_proxies(selected_proxies, format_choice, unfiltered_filename, False)
            print(f"æœªè¿‡æ»¤çš„ä»£ç† IP å·²ä¿å­˜ä¸º {unfiltered_filename}")
            break
        else:
            print("è¾“å…¥æ— æ•ˆï¼Œè¯·è¾“å…¥ 'json' æˆ– 'txt'ã€‚")

    # è¯¢é—®æ£€æµ‹é€‰é¡¹
    check_region = input("æ˜¯å¦éœ€è¦æ£€æµ‹ä»£ç† IP çš„åœ°åŒºä¿¡æ¯ï¼Ÿ(y/n): ").strip().lower() == 'y'
    check_type = input("æ˜¯å¦éœ€è¦æ£€æµ‹ä»£ç†ç±»å‹ï¼Ÿ(y/n): ").strip().lower() == 'y'
    check_https = input("æ˜¯å¦éœ€è¦æ£€æµ‹æ˜¯å¦æ”¯æŒ HTTPSï¼Ÿ(y/n): ").strip().lower() == 'y'
    only_check_availability = not (check_region or check_type or check_https)

    # è¡¥å……æ£€æµ‹ä¿¡æ¯
    print("\nğŸ” æ­£åœ¨åˆ†æä»£ç†ä¿¡æ¯...")
    with concurrent.futures.ThreadPoolExecutor() as executor:
        analyzed_proxies = list(
            executor.map(lambda p: analyze_proxy(p, check_region, check_type, check_https), selected_proxies))

    # éªŒè¯å¯ç”¨æ€§
    print("\nâš¡ æ­£åœ¨éªŒè¯ä»£ç†å¯ç”¨æ€§...")
    with concurrent.futures.ThreadPoolExecutor() as executor:
        valid_proxies = list(filter(None, executor.map(validate_proxy, analyzed_proxies)))

    # ä¿å­˜æœ‰æ•ˆä»£ç† IP
    while True:
        format_choice = input("è¯·é€‰æ‹©æœ‰æ•ˆä»£ç† IP çš„è¾“å‡ºæ ¼å¼ (json/txt): ").strip().lower()
        if format_choice in ['json', 'txt']:
            output_filename = f'valid_proxies_{current_time}.{format_choice}'
            if format_choice == 'txt':
                only_ip = input("æ˜¯å¦ä»…è¾“å‡ºä»£ç† IPï¼Œæ¯è¡Œä¸€ä¸ªä»£ç†ï¼Ÿ(y/n): ").strip().lower() == 'y'
            else:
                only_ip = False
            save_proxies(valid_proxies, format_choice, output_filename, only_ip)
            print(f"\nğŸ‰ æ‰¾åˆ° {len(valid_proxies)} ä¸ªæœ‰æ•ˆä»£ç†ï¼Œå·²ä¿å­˜åˆ° {output_filename}")
            break
        else:
            print("è¾“å…¥æ— æ•ˆï¼Œè¯·è¾“å…¥ 'json' æˆ– 'txt'ã€‚")


if __name__ == "__main__":
    main()
